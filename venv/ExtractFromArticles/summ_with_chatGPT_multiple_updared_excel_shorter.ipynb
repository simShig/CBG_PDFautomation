{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'download_loader' from 'llama_index' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m download_loader\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mopenai\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'download_loader' from 'llama_index' (unknown location)"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from llama_index import download_loader\n",
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "ImageReader = download_loader(\"ImageReader\")\n",
    "imageLoader = ImageReader(text_type=\"plain_text\")\n",
    "FlatPdfReader = download_loader(\"FlatPdfReader\")\n",
    "pdfLoader = FlatPdfReader(image_loader=imageLoader)\n",
    "\n",
    "# paper_list = ['Shapira_Phantom_Sponges_Exploiting_Non-Maximum_Suppression_To_Attack_Deep_Object_Detectors_WACV_2023_paper.pdf','Sponge_Examples_Energy-Latency_Attacks_on_Neural_Networks.pdf',\n",
    "#\n",
    "#                '2211.08859.pdf','1910.11099.pdf']\n",
    "\n",
    "arr_paper = os.listdir(fr'C:\\Users\\Administrator\\Documents\\phd\\oran\\down_paper\\CBG_PDFautomation\\PDFs_new')\n",
    "\n",
    "#arr_paper = ['41_SpongeExamplesEnergyLatencyAttacksonNeuralNetworks.pdf','44_ENERGYLATENCYATTACKSVIASPONGEPOISONING.pdf']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T12:24:04.673530800Z",
     "start_time": "2024-06-03T12:24:04.407910500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Effective Usage of the ChatGPT API\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #Step 2: Set your API Key\n",
    "# def get_openai_key_from_file():\n",
    "#     with open('api_key.txt') as f:\n",
    "#         return f.read().strip()\n",
    "# os.environ['OPENAI_API_KEY'] = get_openai_key_from_file()\n",
    "# print(openai.api_key)\n",
    "# openai.api_key = get_openai_key_from_file()\n",
    "# print(get_openai_key_from_file())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-03T12:24:04.673530800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Step 3: Create a ChatGPT Response Function\n",
    "def get_completion(prompt_sys, prompt_user, model=\"gpt-3.5-turbo-16k\"):#\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt_sys},\n",
    "                {\"role\": \"user\", \"content\": prompt_user}]\n",
    "    #messages = [{\"role\": \"system\", \"content\":\"who won the world series i 2020?\"}]\n",
    "    response = openai.ChatCompletion.create(#\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        #stream=True\n",
    "    )\n",
    "    return response#.choices[0].message[\"content\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T12:24:04.690536200Z",
     "start_time": "2024-06-03T12:24:04.680204500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import base64\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "images_paper_path = fr'C:\\Users\\Administrator\\Documents\\phd\\oran\\papers\\as_images'\n",
    "\n",
    "def pdf_to_images(mypdf):\n",
    "    pdf_images = convert_from_path(mypdf,30,poppler_path=r'C:\\Program Files\\poppler-23.11.0\\Library\\bin')\n",
    "    pdf_images_names_list = []\n",
    "\n",
    "    pdf_name = mypdf.split('\\\\')[-1].replace(\".pdf\",\"\")\n",
    "    path_for_p = fr\"{images_paper_path}\\{pdf_name}\"\n",
    "\n",
    "    if not os.path.exists(path_for_p):\n",
    "        os.makedirs(path_for_p)\n",
    "    for idx in range(len(pdf_images)):\n",
    "        pdf_images[idx].save(fr\"{path_for_p}\\pdf_page_\"+ str(idx+1) +'.png', 'PNG')\n",
    "        pdf_images_names_list.append(fr\"{path_for_p}\\pdf_page_\"+ str(idx+1) +'.png')\n",
    "    print(\"Successfully converted PDF to images\")\n",
    "    return pdf_images_names_list\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def pdf2gptmessage_content(mypdf):\n",
    "    pdf_images_names_list = pdf_to_images(mypdf)\n",
    "\n",
    "    content = []\n",
    "    for image_name in pdf_images_names_list:\n",
    "        content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(image_name)}\"}})\n",
    "    return content\n",
    "\n",
    "\n",
    "    return pdf_images_base64_list\n",
    "\n",
    "demi_user2_base64_images_contene = pdf2gptmessage_content(fr'C:\\Users\\Administrator\\Documents\\phd\\oran\\papers\\{paper}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-03T12:24:04.680204500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://avishag.openai.azure.com/\"  # os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_key = \"de53e63870474486910d4109044e9be3\"  # os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "openai.api_version = \"2023-12-01\"\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=\"de53e63870474486910d4109044e9be3\",  # os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=\"2023-12-01-preview\",\n",
    "    azure_endpoint=\"https://avishag.openai.azure.com/\"  # os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    ")\n",
    "\n",
    "deployment_name = 'gpt4-avishag'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-03T12:24:04.680204500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Step 3: Create a ChatGPT Response Function\n",
    "def get_completion_old(prompt_sys, prompt_user, model=\"gpt-4-1106-preview\" ):#\"gpt-3.5-turbo-16k\"):#\"gpt-3.5-turbo-16k-0613\"):#:#\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt_sys},\n",
    "                #{\"role\": \"user\", \"content\": demi_user1.text},\n",
    "                #{\"role\": \"assistant\", \"content\": assistant_string1},\n",
    "                {\"role\": \"user\", \"content\": demi_user2.text},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_string2},\n",
    "                {\"role\": \"user\", \"content\": prompt_user}]\n",
    "    #messages = [{\"role\": \"system\", \"content\":\"who won the world series i 2020?\"}]\n",
    "    response = client.chat.completions.create(#\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        #response_format={ \"type\": \"json_object\" }\n",
    "        #stream=True\n",
    "    )\n",
    "    return response#.choices[0].message[\"content\"]\n",
    "\n",
    "def get_completion(prompt_sys, prompt_user, model=\"gpt-4-1106-preview\" ):#\"gpt-3.5-turbo-16k\"):#\"gpt-3.5-turbo-16k-0613\"):#:#\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt_sys},\n",
    "\n",
    "                {\"role\": \"user\", \"content\": prompt_user}]\n",
    "    #messages = [{\"role\": \"system\", \"content\":\"who won the world series i 2020?\"}]\n",
    "    response = client.chat.completions.create(#\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        #response_format={ \"type\": \"json_object\" }\n",
    "        #stream=True\n",
    "    )\n",
    "    return response#.choices[0].message[\"content\"]\n",
    "\n",
    "def get_completion_images(prompt_sys, prompt_user_base64_image, model=\"gpt-4-vision-preview\" ):#\"gpt-3.5-turbo-16k\"):#\"gpt-3.5-turbo-16k-0613\"):#:#\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"system\", \"content\": [{\"type\":\"text\", \"text\": prompt_sys}]},\n",
    "                #{\"role\": \"user\", \"content\": demi_user1.text},\n",
    "                #{\"role\": \"assistant\", \"content\": assistant_string1},\n",
    "                {\"role\": \"user\", \"content\": demi_user2_base64_images_contene},\n",
    "                {\"role\": \"assistant\", \"content\": [{\"type\":\"text\", \"text\": assistant_string2}]},\n",
    "                {\"role\": \"user\", \"content\": prompt_user_base64_image},\n",
    "                #{\"type\": \"image_url\",\"image_url\": {\"url\": f\"data:image/jpeg;base64,{prompt_user_base64_image}\"}}}\n",
    "                ]\n",
    "\n",
    "    #print(messages)\n",
    "    #messages = [{\"role\": \"system\", \"content\":\"who won the world series i 2020?\"}]\n",
    "    response = client.chat.completions.create(#\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens = 4096,\n",
    "        #response_format={ \"type\": \"json_object\" }\n",
    "        #stream=True\n",
    "    )\n",
    "    return response#.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "def get_completion_only_sys(prompt_sys, prompt_user, model=\"gpt-4-1106-preview\" ):#\"gpt-3.5-turbo-16k\"):#\"gpt-3.5-turbo-16k-0613\"):#:#\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt_sys},\n",
    "\n",
    "                {\"role\": \"user\", \"content\": prompt_user}]\n",
    "    #messages = [{\"role\": \"system\", \"content\":\"who won the world series i 2020?\"}]\n",
    "    response = client.chat.completions.create(#\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        #response_format={ \"type\": \"json_object\" }\n",
    "        #stream=True\n",
    "    )\n",
    "    return response#.choices[0].message[\"content\"]\n",
    "\n",
    "#\n",
    "def get_completion_validation(prompt_sys, prompt_user_paper, prompt_assis, prompt_user_valid, model=\"gpt-4-1106-preview\" ):#\"gpt-3.5-turbo-16k\"):#\"gpt-3.5-turbo-16k-0613\"):#:#\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt_sys},\n",
    "                #{\"role\": \"user\", \"content\": demi_user1.text},\n",
    "                #{\"role\": \"assistant\", \"content\": assistant_string1},\n",
    "                {\"role\": \"user\", \"content\": demi_user2.text},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_string2},\n",
    "                {\"role\": \"user\", \"content\": prompt_user_paper},\n",
    "                 {\"role\": \"user\", \"content\": prompt_assis},\n",
    "                {\"role\": \"user\", \"content\": prompt_user_valid}]\n",
    "    #messages = [{\"role\": \"system\", \"content\":\"who won the world series i 2020?\"}]\n",
    "    response = client.chat.completions.create(#\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        #response_format={ \"type\": \"json_object\" }\n",
    "        #stream=True\n",
    "    )\n",
    "    return response#.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T12:24:04.690536200Z",
     "start_time": "2024-06-03T12:24:04.690536200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Step 4: Query the API\n",
    "def get_template():\n",
    "    with open('summary_template_system_updated_excel.txt') as f:#with open('summary_template_system_new_3_gpt4-turbo-only-part.txt') as f:#summary_template_system_new_3_gpt4-turbo.txt', 'r') as f:#open('summary_template_system_new_3_gpt4-turbo-only-part.txt', 'r') as f:#('summary_template_system_new_3_gpt4-turbo-5arracks_looks good for sponge exampple.txt', 'r') as f:###('summary_template_system_new_3_gpt4-turbo-only-few-fields.txt', 'r') as f:#\n",
    "        return f.read().strip()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-03T12:24:04.690536200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#step 5: converto to json\n",
    "#C:\\Users\\Administrator\\Documents\\phd\\oran\\papers\n",
    "import json\n",
    "\n",
    "def text_to_json(text):\n",
    "    lines = text.strip().split('\\n')\n",
    "    result_dict = {}\n",
    "\n",
    "    for line in lines:\n",
    "        # Split each line by the first colon to separate the key and value\n",
    "        parts = line.split(\":\", 1)\n",
    "\n",
    "        if len(parts) == 2:\n",
    "            key, value = parts\n",
    "            result_dict[key.strip()] = value.strip()\n",
    "\n",
    "    return json.dumps(result_dict, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-03T12:24:04.690536200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install --upgrade h5py\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-03T12:24:04.690536200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from llama_hub.nougat_ocr import PDFNougatOCR\n",
    "# import os\n",
    "#\n",
    "# from pathlib import *\n",
    "# reader = PDFNougatOCR()\n",
    "#\n",
    "# pdf_path = Path(fr'./../108_AdvHatRealWorldAdversarialAttackonArcFaceFaceIDSystem.pdf')\n",
    "#\n",
    "# documents = reader.load_data(pdf_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-03T12:24:04.690536200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# arr_paper = ['108_AdvHatRealWorldAdversarialAttackonArcFaceFaceIDSystem.pdf']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-03T12:24:04.690536200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###go over all the papers until final analysis\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "prompt_sys = \"\"\n",
    "paper_list_first_analysis = []\n",
    "paper_list_sec_analysis = []\n",
    "json_lists =[]\n",
    "columns_order = [\"paper_name\",\"Attack Name\", \"Objective\", \"Threat Model\", \"Attacker Knowledge\",  \"is Backdoor\" , \"Attack Type\", \"Attack Phase\",\n",
    "                 \"Field\", \"Applicable in the physical domain\", \"Description\", \"Attack's success rate - digital domain\", \"Attack's success rate - physical domain\", \"Attacked models\", \"Datasets\", \"Reference\"]\n",
    "df_columns_order = pd.DataFrame([columns_order], columns=columns_order)\n",
    "\n",
    "csv_name = 'output_new_updated.csv'\n",
    "if not os.path.exists(csv_name):\n",
    "    df_columns_order.to_csv(csv_name, mode='a', index=False, header=False)\n",
    "    df_existing = pd.DataFrame(columns=columns_order)\n",
    "else:\n",
    "    df_existing = pd.read_csv(csv_name, header=None, names=columns_order)\n",
    "\n",
    "existing = list(df_existing['paper_name'].values)\n",
    "\n",
    "for paper_name in arr_paper:#paper_list:\n",
    "    print(\"paper name: \"+paper_name)\n",
    "    # Iterate through the new data and add rows that don't exist in the existing CSV\n",
    "    if paper_name in existing:#df_existing['paper_name'].values:  # Assuming 'name' is the attack name\n",
    "        continue\n",
    "    existing.append(paper_name)\n",
    "    document = pdfLoader.load_data(file=Path(fr'C:\\Users\\Administrator\\Documents\\phd\\oran\\down_paper\\CBG_PDFautomation\\PDFs_new\\{paper_name}'))#(file=Path(fr'C:\\Users\\Administrator\\Documents\\phd\\oran\\papers\\{paper_name}'))\n",
    "\n",
    "    #print(document.text)\n",
    "\n",
    "    prompt_sys = get_template()#\"<YOUR QUERY>\"\n",
    "    #print(prompt_sys)\n",
    "    response = get_completion(prompt_sys, document.text)#get_completion_images(prompt_sys, pdf2gptmessage_content(fr'C:\\Users\\Administrator\\Documents\\phd\\oran\\papers\\{paper_name}'))#\n",
    "    #print(response)\n",
    "\n",
    "    #json_output = text_to_json(response.choices[0].message[\"content\"])\n",
    "    paper_f_ana = response.choices[0].message[\"content\"]\n",
    "    print(paper_f_ana)\n",
    "\n",
    "    paper_list_first_analysis.append(paper_f_ana)#(json_output)\n",
    "    ##print(response.choices[0].message[\"content\"])#json_output)\n",
    "    print(\"---------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    ##sec analysis (fine tuning)\n",
    "    prompt_user_valid = \"You are a system that refines and clarifies information. Every time, the system gets a summarization on an attack in a JSON format, and adjusts the info that is is presented to be an input for another automatic analysis system. \" \\\n",
    "    \"The system tries to refine the information in the original JSON, make it shorter and more accurate. \" \\\n",
    "                 \"Here are some EXAMPLES:\" \\\n",
    "                 \"1. if you get a range of numbers - the system replaces it with the average of the range. \" \\\n",
    "                 \"2. if data is presented with many words, the system tries to replace it with the most important info (for example, with a number if possible). \" \\\n",
    "                 \"3. there are 'Description' and 'explanation' fields. These fields contain info regarding other fields (specially, about the 'Attack's success rate' fields and 'Attack Type'). \" \\\n",
    "                 \"The information in these fields is more reliable than values in the other fields. \" \\\n",
    "                 \"Analyze CAREFULLY the info in the 'Description' and 'explanation' fields, in cases where you can fill in other fields' values based on the info you found during the analysis of those fields - replace the exists values in the other fields with the info that was extracted from the 'Description' and 'explanation' fields.\" \\\n",
    "                 \"For example, if the info in the existing 'attack's success rate - digital domain' is different between than the information that was extracted from the 'explanation' fields - change the value of the 'Attack's success rate - digital domain' to be the same as in the 'explanation' field).\"##. at the end, don't add the 'explanation' field to the final response\"####\"Therefore, examine if there is contradict info between the info in this field and others, and prioritize info from this fields (for example, if the attack's success rate are different between the explanation and the 'Attack's success rate' value - change the inf oin 'Attack's success rate' to be the same as in the 'explanation' field).\" \\\n",
    "\n",
    "\n",
    "    #print(prompt_sys)\n",
    "    prompt_user_paper = document.text\n",
    "    #(prompt_sys, prompt_user_paper, prompt_assis, prompt_user_valid,\n",
    "    response = get_completion_validation(prompt_sys, prompt_user_paper, paper_f_ana,prompt_user_valid)\n",
    "    #print(response)\n",
    "\n",
    "    #json_output = text_to_json(response.choices[0].message[\"content\"])\n",
    "    paper_list_sec_analysis.append(response.choices[0].message[\"content\"])\n",
    "    res_sec_ana = response.choices[0].message[\"content\"]\n",
    "    print(res_sec_ana)\n",
    "    print(\"********************************************\")\n",
    "    prompt_user_valid_by_model = \"what are the attacks' success rate (digital and physical) and what are the types of attacks of all the attacks tested on the LResNet100E model? (based on the syste instructions)\"\n",
    "    response_by_model = get_completion_validation(prompt_sys, prompt_user_paper, \"lets say that you got an answer\",prompt_user_valid_by_model)\n",
    "    print(response_by_model.choices[0].message[\"content\"])\n",
    "    print(\"********************************************\")\n",
    "\n",
    "# Extracting JSON strings\n",
    "    #json_string = re.findall(r'```json\\n([\\s\\S]*?)\\n```',res_sec_ana)\n",
    "    # Regular expression for matching JSON objects and arrays\n",
    "    json_regex = r'(\\{.*?\\}|\\[.*?\\])'\n",
    "\n",
    "    # Find all matches in the text\n",
    "    json_string = re.findall(json_regex, res_sec_ana, re.DOTALL)\n",
    "\n",
    "    # Converting JSON strings to Python lists of dictionaries\n",
    "    #if len(json_string) != 0:\n",
    "        #json_strings = re.findall(r'```json\\n([\\s\\S]*?)```',res_sec_ana)\n",
    "\n",
    "\n",
    "    '''\n",
    "    # Converting JSON strings to Python lists of dictionaries\n",
    "    try:\n",
    "        json_list = [json.loads(js)[0] for js in json_strings]#[json.loads(js) for js in json_strings]\n",
    "    except:\n",
    "        json_list = [json.loads(js) for js in json_strings]\n",
    "    '''\n",
    "\n",
    "        # Parse each match as JSON\n",
    "    json_list = []\n",
    "    for match in json_string:\n",
    "        try:\n",
    "            json_object = json.loads(match)\n",
    "            json_list+=(json_object)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Invalid JSON detected and skipped:\", match)\n",
    "\n",
    "    json_lists+=(json_list)\n",
    "\n",
    "    #all_jsons+=json_list[0]\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(json_list)\n",
    "    df['paper_name'] = paper_name\n",
    "\n",
    "    df = df.reindex(columns=columns_order)\n",
    "\n",
    "    # Iterate through the new data and add rows that don't exist in the existing CSV\n",
    "\n",
    "    # Fill NaN values with a default value or leave as is\n",
    "    df.fillna(\"\", inplace=True)  # Replace NaN with empty string\n",
    "    # Write the DataFrame to an Excel file\n",
    "    df.to_csv(csv_name, mode='a', index=False, header=False)\n",
    "\n",
    "    print(\"********************************************\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-03T12:24:04.690536200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-03T12:24:04.690536200Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
